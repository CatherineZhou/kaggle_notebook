{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89f2ac8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-10-12T09:39:48.365181Z",
     "iopub.status.busy": "2022-10-12T09:39:48.364668Z",
     "iopub.status.idle": "2022-10-12T09:39:48.512012Z",
     "shell.execute_reply": "2022-10-12T09:39:48.511033Z"
    },
    "papermill": {
     "duration": 0.160187,
     "end_time": "2022-10-12T09:39:48.518348",
     "exception": false,
     "start_time": "2022-10-12T09:39:48.358161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/scriptpretrain-ngram/pretrain/train_bert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/NLP_Utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_pytorch_checkpoint_to_tf2.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/training_args_tf.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_ctrl.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_flaubert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/hf_argparser.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_auto.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_reformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/trainer.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/benchmark_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_electra.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_xlm.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_t5.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_xlnet.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_xlm_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/file.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_electra.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_electra.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_gpt2.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_encoder_decoder.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/training_args.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_distilbert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_bert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/filep.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_mmbt.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_ctrl.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_bert_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_flaubert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_mmbt.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_auto.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/file_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/hf_api.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_bert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modelcard.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_albert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_electra.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_reformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_openai.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_albert_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_distilbert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_xlnet.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_flaubert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_camembert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/trainer_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_longformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_xlnet.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_bert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_marian.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_transfo_xl.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_transfo_xl_utilities.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_bart.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_encoder_decoder.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_gpt2.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_longformer_original_pytorch_lightning_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_electra_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_auto.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_graph_to_onnx.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_longformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_marian_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_camembert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_flaubert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_transfo_xl_utilities.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/activations.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_openai_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_pytorch_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_transfo_xl.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_xlm_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_albert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_camembert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_camembert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_bert_pytorch_checkpoint_to_original_tf.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_openai.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/trainer_tf.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/utils_encoder_decoder.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_gpt2.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/optimization.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/optimization_tf.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/try.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_ctrl.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_gpt2.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_longformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_t5.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_bart_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_t5.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/another_try.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_xlm_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/pipelines.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_bert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_t5.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_distilbert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_auto.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_xlm.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_distilbert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_openai.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_marian.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_albert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_xlm_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_roberta_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_albert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_transfo_xl.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_transfo_xl.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_gpt2_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_xlnet_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_reformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_reformer_trax_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_ctrl.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_marian.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/__main__.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_openai.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/convert_t5_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_beam_search.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/configuration_xlm_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_xlnet.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_bert_japanese.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_bart.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_xlm.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/modeling_tf_xlm.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/tokenization_bart.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/commands/train.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/commands/run.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/commands/env.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/commands/convert.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/commands/user.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/commands/serving.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/commands/download.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/commands/transformers_cli.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/commands/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/benchmark/benchmark_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/benchmark/benchmark_args_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/benchmark/benchmark.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/benchmark/benchmark_args.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/benchmark/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/data_collator.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/processors/xnli.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/processors/utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/processors/squad.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/processors/glue.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/processors/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/datasets/language_modeling.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/datasets/glue.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/datasets/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/metrics/squad_metrics.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/transformers1/data/metrics/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/pretrain/bert_model/gitkeep\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/NLP_Utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/train_nezha.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_pytorch_checkpoint_to_tf2.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/training_args_tf.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_ctrl.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_flaubert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/hf_argparser.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_auto.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_reformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/trainer.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/benchmark_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_electra.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_xlm.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_t5.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_xlnet.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_xlm_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/file.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_electra.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_electra.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_gpt2.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_encoder_decoder.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/training_args.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_distilbert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_bert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/filep.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_mmbt.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_ctrl.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_bert_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_flaubert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_mmbt.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_auto.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/file_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/hf_api.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_bert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modelcard.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_albert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_electra.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_reformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_openai.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_albert_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_distilbert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_xlnet.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_flaubert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_camembert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/trainer_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_longformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_xlnet.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_bert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_marian.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_transfo_xl.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_transfo_xl_utilities.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_bart.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_encoder_decoder.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_gpt2.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_longformer_original_pytorch_lightning_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_electra_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_auto.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_graph_to_onnx.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_longformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_marian_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_camembert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_flaubert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_transfo_xl_utilities.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/activations.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_openai_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_pytorch_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_transfo_xl.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_xlm_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_albert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_camembert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_camembert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_bert_pytorch_checkpoint_to_original_tf.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_openai.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/trainer_tf.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/utils_encoder_decoder.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_gpt2.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/optimization.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/optimization_tf.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/try.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_ctrl.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_gpt2.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_longformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_t5.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_bart_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_t5.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/another_try.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_xlm_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/pipelines.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_bert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_t5.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_distilbert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_auto.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_xlm.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_distilbert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_openai.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_marian.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_albert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_xlm_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_roberta_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_albert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_transfo_xl.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_transfo_xl.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_gpt2_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_xlnet_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_reformer.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_reformer_trax_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_ctrl.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_marian.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/__main__.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_openai.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/convert_t5_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_beam_search.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/configuration_xlm_roberta.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_xlnet.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_bert_japanese.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_bart.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_xlm.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/modeling_tf_xlm.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/tokenization_bart.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/commands/train.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/commands/run.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/commands/env.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/commands/convert.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/commands/user.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/commands/serving.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/commands/download.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/commands/transformers_cli.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/commands/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/benchmark/benchmark_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/benchmark/benchmark_args_utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/benchmark/benchmark.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/benchmark/benchmark_args.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/benchmark/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/data_collator.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/processors/xnli.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/processors/utils.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/processors/squad.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/processors/glue.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/processors/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/datasets/language_modeling.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/datasets/glue.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/datasets/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/metrics/squad_metrics.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/transformers1/data/metrics/__init__.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/nezha_model/gitkeep\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/NEZHA/configuration_nezha.py\n",
      "/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain/NEZHA/modeling_nezha.py\n",
      "/kaggle/input/competition/merge_match_data_prompt.txt/merge_match_data_prompt.txt\n",
      "/kaggle/input/competition/competition_tag_extract_pretrain_data.txt/competition_tag_extract_pretrain_data.txt\n",
      "/kaggle/input/competition/challenge_all_data.txt/challenge_all_data.txt\n",
      "/kaggle/input/pretrainbertmodel/0921_all_challenge_data/0921_all_challenge_data/config.json\n",
      "/kaggle/input/pretrainbertmodel/0921_all_challenge_data/0921_all_challenge_data/pytorch_model.bin\n",
      "/kaggle/input/pretrainbertmodel/0921_all_challenge_data/0921_all_challenge_data/vocab.txt\n",
      "/kaggle/input/pretrainbertmodel/nezha-cn-base_new/nezha-cn-base/config.json\n",
      "/kaggle/input/pretrainbertmodel/nezha-cn-base_new/nezha-cn-base/pytorch_model.bin\n",
      "/kaggle/input/pretrainbertmodel/nezha-cn-base_new/nezha-cn-base/vocab.txt\n",
      "/kaggle/input/pretrainbertmodel/match_challenge_ngram_pretrain/match_challenge_ngram_pretrain/config.json\n",
      "/kaggle/input/pretrainbertmodel/match_challenge_ngram_pretrain/match_challenge_ngram_pretrain/training_args.bin\n",
      "/kaggle/input/pretrainbertmodel/match_challenge_ngram_pretrain/match_challenge_ngram_pretrain/pytorch_model.bin\n",
      "/kaggle/input/pretrainbertmodel/match_challenge_ngram_pretrain/match_challenge_ngram_pretrain/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ae5a5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T09:39:48.528855Z",
     "iopub.status.busy": "2022-10-12T09:39:48.528225Z",
     "iopub.status.idle": "2022-10-12T09:39:48.532709Z",
     "shell.execute_reply": "2022-10-12T09:39:48.531924Z"
    },
    "papermill": {
     "duration": 0.017659,
     "end_time": "2022-10-12T09:39:48.540329",
     "exception": false,
     "start_time": "2022-10-12T09:39:48.522670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/scriptpretrain-ngram/nezah_script_pretrain/pretrain') # 最后末尾不要加反斜杠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e248b",
   "metadata": {
    "papermill": {
     "duration": 0.004747,
     "end_time": "2022-10-12T09:39:48.550387",
     "exception": false,
     "start_time": "2022-10-12T09:39:48.545640",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f32b927e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T09:39:48.561226Z",
     "iopub.status.busy": "2022-10-12T09:39:48.560867Z",
     "iopub.status.idle": "2022-10-12T09:39:58.286419Z",
     "shell.execute_reply": "2022-10-12T09:39:58.285322Z"
    },
    "papermill": {
     "duration": 9.734098,
     "end_time": "2022-10-12T09:39:58.289029",
     "exception": false,
     "start_time": "2022-10-12T09:39:48.554931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import transformers as _\n",
    "from transformers1 import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34ddb5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T09:39:58.297380Z",
     "iopub.status.busy": "2022-10-12T09:39:58.296411Z",
     "iopub.status.idle": "2022-10-12T09:39:58.984322Z",
     "shell.execute_reply": "2022-10-12T09:39:58.983372Z"
    },
    "papermill": {
     "duration": 0.694702,
     "end_time": "2022-10-12T09:39:58.986701",
     "exception": false,
     "start_time": "2022-10-12T09:39:58.291999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数量： 70001\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import transformers as _\n",
    "from transformers1 import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "def writeToJsonFile(path: str, obj):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False,indent=0))\n",
    "def readFromJsonFile(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "def loadData(path):\n",
    "    allData=[]\n",
    "    with open(path,\"r\") as f:\n",
    "        for i in f:\n",
    "            i=i.strip().split('\\t')\n",
    "            if len(i)==0:#防止空行\n",
    "                break\n",
    "            if len(i)==3:#训练集\n",
    "                a,b,label=i\n",
    "                a=a.split(' ')\n",
    "                b=b.split(' ')\n",
    "            else:#测试集，直接转为id形式\n",
    "                a,b,label=i[0],i[1],-1\n",
    "                a=a.split(' ')\n",
    "                b=b.split(' ')\n",
    "            allData.append([a,b,label])\n",
    "    return allData\n",
    "\n",
    "def calNegPos(ls):#计算正负比例\n",
    "    posNum,negNum=0,0\n",
    "    for i in ls:\n",
    "        if i[2]==0:\n",
    "            negNum+=1\n",
    "        elif i[2]==1:\n",
    "            posNum+=1\n",
    "    posNum=1 if posNum==0 else posNum\n",
    "    return negNum,posNum,round(negNum/posNum,4)\n",
    "\n",
    "data_dir = '/kaggle/input/competition/challenge_all_data.txt'\n",
    "allData=loadData(data_dir + '/challenge_all_data.txt')\n",
    "random.shuffle(allData)\n",
    "\n",
    "train_data=allData\n",
    "valid_data=allData[-1000:]\n",
    "print(\"训练集样本数量：\", len(train_data))\n",
    "\n",
    "def paddingList(ls:list,val,returnTensor=False):\n",
    "    ls=ls[:]#不要改变了原list尺寸\n",
    "    maxLen=max([len(i) for i in ls])\n",
    "    for i in range(len(ls)):\n",
    "        ls[i]=ls[i]+[val]*(maxLen-len(ls[i]))\n",
    "    return torch.tensor(ls,device='cuda') if returnTensor else ls\n",
    "\n",
    "def truncate(a:list,b:list,maxLen):\n",
    "    maxLen-=3#空留给cls sep sep\n",
    "    assert maxLen>=0\n",
    "    len2=maxLen//2#若为奇数，更长部分给左边\n",
    "    len1=maxLen-len2\n",
    "    #一共就a超长与否，b超长与否，组合的四种情况\n",
    "    if len(a)+len(b)>maxLen:#需要截断\n",
    "        if len(a)<=len1 and len(b)>len2:\n",
    "            b=b[:maxLen-len(a)]\n",
    "        elif len(a)>len1 and len(b)<=len2:\n",
    "            a=a[:maxLen-len(b)]\n",
    "        elif len(a)>len1 and len(b)>len2:\n",
    "            a=a[:len1]\n",
    "            b=b[:len2]\n",
    "    return a,b\n",
    "\n",
    "class MLM_Data(Dataset):\n",
    "    #传入句子对列表\n",
    "    def __init__(self,textLs:list,maxLen:int,tk:BertTokenizer):\n",
    "        super().__init__()\n",
    "        self.data=textLs\n",
    "        self.maxLen=maxLen\n",
    "        self.tk=tk\n",
    "        self.spNum=len(tk.all_special_tokens)\n",
    "        self.tkNum=tk.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def random_mask(self,text_ids):\n",
    "        input_ids, output_ids = [], []\n",
    "        rands = np.random.random(len(text_ids))\n",
    "        idx=0\n",
    "        while idx<len(rands):\n",
    "            if rands[idx]<0.15:#需要mask\n",
    "                ngram=np.random.choice([1,2,3], p=[0.7,0.2,0.1])#若要mask，进行x_gram mask的概率\n",
    "                if ngram==3 and len(rands)<7:#太大的gram不要应用于过短文本\n",
    "                    ngram=2\n",
    "                if ngram==2 and len(rands)<4:\n",
    "                    ngram=1\n",
    "                L=idx+1\n",
    "                R=idx+ngram#最终需要mask的右边界（开）\n",
    "                while L<R and L<len(rands):\n",
    "                    rands[L]=np.random.random()*0.15#强制mask\n",
    "                    L+=1\n",
    "                idx=R\n",
    "                if idx<len(rands):\n",
    "                    rands[idx]=1#禁止mask片段的下一个token被mask，防止一大片连续mask\n",
    "            idx+=1\n",
    "\n",
    "        for r, i in zip(rands, text_ids):\n",
    "            if r < 0.15 * 0.8:\n",
    "                input_ids.append(self.tk.mask_token_id)\n",
    "                output_ids.append(i)#mask预测自己\n",
    "            elif r < 0.15 * 0.9:\n",
    "                input_ids.append(i)\n",
    "                output_ids.append(i)#自己预测自己\n",
    "            elif r < 0.15:\n",
    "                input_ids.append(np.random.randint(self.spNum,self.tkNum))\n",
    "                output_ids.append(i)#随机的一个词预测自己，随机词不会从特殊符号中选取，有小概率抽到自己\n",
    "            else:\n",
    "                input_ids.append(i)\n",
    "                output_ids.append(-100)#保持原样不预测\n",
    "\n",
    "        return input_ids, output_ids\n",
    "\n",
    "    #耗时操作在此进行，可用上多进程\n",
    "    def __getitem__(self, item):\n",
    "        text1,text2,_=self.data[item]#预处理，mask等操作\n",
    "        if random.random()>0.5:\n",
    "            text1,text2=text2,text1#交换位置\n",
    "        text1,text2=truncate(text1,text2,self.maxLen)\n",
    "        text1_ids,text2_ids = self.tk.convert_tokens_to_ids(text1),self.tk.convert_tokens_to_ids(text2)\n",
    "        text1_ids, out1_ids = self.random_mask(text1_ids)#添加mask预测\n",
    "        text2_ids, out2_ids = self.random_mask(text2_ids)\n",
    "        input_ids = [self.tk.cls_token_id] + text1_ids + [self.tk.sep_token_id] + text2_ids + [self.tk.sep_token_id]#拼接\n",
    "        token_type_ids=[0]*(len(text1_ids)+2)+[1]*(len(text2_ids)+1)\n",
    "        labels = [-100] + out1_ids + [-100] + out2_ids + [-100]\n",
    "        assert len(input_ids)==len(token_type_ids)==len(labels)\n",
    "        return {'input_ids':input_ids,'token_type_ids':token_type_ids,'labels':labels}\n",
    "\n",
    "    @classmethod\n",
    "    def collate(cls,batch):\n",
    "        input_ids=[i['input_ids'] for i in batch]\n",
    "        token_type_ids=[i['token_type_ids'] for i in batch]\n",
    "        labels=[i['labels'] for i in batch]\n",
    "        input_ids=paddingList(input_ids,0,returnTensor=True)\n",
    "        token_type_ids=paddingList(token_type_ids,0,returnTensor=True)\n",
    "        labels=paddingList(labels,-100,returnTensor=True)\n",
    "        attention_mask=(input_ids!=0)\n",
    "        return {'input_ids':input_ids,'token_type_ids':token_type_ids\n",
    "                ,'attention_mask':attention_mask,'labels':labels}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "unionList=lambda ls:list(chain(*ls))#按元素拼接\n",
    "splitList=lambda x,bs:[x[i:i+bs] for i in range(0,len(x),bs)]#按bs切分\n",
    "\n",
    "\n",
    "#sortBsNum：原序列按多少个bs块为单位排序，可用来增强随机性\n",
    "#比如如果每次打乱后都全体一起排序，那每次都是一样的\n",
    "def blockShuffle(data:list,bs:int,sortBsNum,key):\n",
    "    random.shuffle(data)#先打乱\n",
    "    tail=len(data)%bs#计算碎片长度\n",
    "    tail=[] if tail==0 else data[-tail:]\n",
    "    data=data[:len(data)-len(tail)]\n",
    "    assert len(data)%bs==0#剩下的一定能被bs整除\n",
    "    sortBsNum=len(data)//bs if sortBsNum is None else sortBsNum#为None就是整体排序\n",
    "    data=splitList(data,sortBsNum*bs)\n",
    "    data=[sorted(i,key=key,reverse=True) for i in data]#每个大块进行降排序\n",
    "    data=unionList(data)\n",
    "    data=splitList(data,bs)#最后，按bs分块\n",
    "    random.shuffle(data)#块间打乱\n",
    "    data=unionList(data)+tail\n",
    "    return data\n",
    "from torch.utils.data.dataloader import _SingleProcessDataLoaderIter,_MultiProcessingDataLoaderIter\n",
    "#每轮迭代重新分块shuffle数据的DataLoader\n",
    "class blockShuffleDataLoader(DataLoader):\n",
    "    def __init__(self, dataset: Dataset,sortBsNum,key,**kwargs):\n",
    "        assert isinstance(dataset.data,list)#需要有list类型的data属性\n",
    "        super().__init__(dataset,**kwargs)#父类的参数传过去\n",
    "        self.sortBsNum=sortBsNum\n",
    "        self.key=key\n",
    "\n",
    "    def __iter__(self):\n",
    "        #分块shuffle\n",
    "        self.dataset.data=blockShuffle(self.dataset.data,self.batch_size,self.sortBsNum,self.key)\n",
    "        if self.num_workers == 0:\n",
    "            return _SingleProcessDataLoaderIter(self)\n",
    "        else:\n",
    "            return _MultiProcessingDataLoaderIter(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e26f30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T09:39:58.994215Z",
     "iopub.status.busy": "2022-10-12T09:39:58.993922Z",
     "iopub.status.idle": "2022-10-12T16:21:29.625295Z",
     "shell.execute_reply": "2022-10-12T16:21:29.624283Z"
    },
    "papermill": {
     "duration": 24090.638054,
     "end_time": "2022-10-12T16:21:29.627988",
     "exception": false,
     "start_time": "2022-10-12T09:39:58.989934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/pretrainbertmodel/nezha-cn-base_new/nezha-cn-base/ were not used when initializing NeZhaForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing NeZhaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py:813: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"loss\": 0.44194976579368334, \"learning_rate\": 1.7288242730720608e-06, \"epoch\": 1.0, \"step\": 547}\n",
      "\n",
      "预训练第0轮耗时： 61.71517848968506\n",
      "\n",
      "{\"loss\": 0.0001675276073911352, \"learning_rate\": 3.4576485461441217e-06, \"epoch\": 2.0, \"step\": 1094}\n",
      "\n",
      "预训练第1轮耗时： 60.227346420288086\n",
      "\n",
      "{\"loss\": 1.2032099880218892e-06, \"learning_rate\": 5.186472819216182e-06, \"epoch\": 3.0, \"step\": 1641}\n",
      "\n",
      "预训练第2轮耗时： 60.28002977371216\n",
      "\n",
      "{\"loss\": 5.702512383728453e-07, \"learning_rate\": 6.915297092288243e-06, \"epoch\": 4.0, \"step\": 2188}\n",
      "\n",
      "预训练第3轮耗时： 60.19330620765686\n",
      "\n",
      "{\"loss\": 1.6582422256534637e-07, \"learning_rate\": 8.644121365360304e-06, \"epoch\": 5.0, \"step\": 2735}\n",
      "\n",
      "预训练第4轮耗时： 60.29735732078552\n",
      "\n",
      "{\"loss\": 2.703041681419234e-06, \"learning_rate\": 1.0372945638432365e-05, \"epoch\": 6.0, \"step\": 3282}\n",
      "\n",
      "预训练第5轮耗时： 60.15055584907532\n",
      "\n",
      "{\"loss\": 1.3903436257256358e-09, \"learning_rate\": 1.2101769911504425e-05, \"epoch\": 7.0, \"step\": 3829}\n",
      "\n",
      "预训练第6轮耗时： 60.414249897003174\n",
      "\n",
      "{\"loss\": 6.639828317474062e-10, \"learning_rate\": 1.3830594184576487e-05, \"epoch\": 8.0, \"step\": 4376}\n",
      "\n",
      "预训练第7轮耗时： 59.92383885383606\n",
      "\n",
      "{\"loss\": 6.954079936310371e-10, \"learning_rate\": 1.555941845764855e-05, \"epoch\": 9.0, \"step\": 4923}\n",
      "\n",
      "预训练第8轮耗时： 60.18775391578674\n",
      "\n",
      "{\"loss\": 4.5609104825190456e-09, \"learning_rate\": 1.728824273072061e-05, \"epoch\": 10.0, \"step\": 5470}\n",
      "\n",
      "预训练第9轮耗时： 60.16507411003113\n",
      "\n",
      "{\"loss\": 3.7961131455410267e-10, \"learning_rate\": 1.901706700379267e-05, \"epoch\": 11.0, \"step\": 6017}\n",
      "\n",
      "预训练第10轮耗时： 60.352890729904175\n",
      "\n",
      "{\"loss\": 3.528072004765091e-10, \"learning_rate\": 2.074589127686473e-05, \"epoch\": 12.0, \"step\": 6564}\n",
      "\n",
      "预训练第11轮耗时： 60.14160180091858\n",
      "\n",
      "{\"loss\": 2.7378222216237795e-10, \"learning_rate\": 2.247471554993679e-05, \"epoch\": 13.0, \"step\": 7111}\n",
      "\n",
      "预训练第12轮耗时： 60.15907287597656\n",
      "\n",
      "{\"loss\": 1.2187280963006655e-10, \"learning_rate\": 2.420353982300885e-05, \"epoch\": 14.0, \"step\": 7658}\n",
      "\n",
      "预训练第13轮耗时： 60.07322287559509\n",
      "\n",
      "{\"loss\": 1.0264762754999957e-10, \"learning_rate\": 2.5932364096080913e-05, \"epoch\": 15.0, \"step\": 8205}\n",
      "\n",
      "预训练第14轮耗时： 60.64783310890198\n",
      "\n",
      "{\"loss\": 4.496361195214388e-11, \"learning_rate\": 2.7661188369152973e-05, \"epoch\": 16.0, \"step\": 8752}\n",
      "\n",
      "预训练第15轮耗时： 60.13796544075012\n",
      "\n",
      "{\"loss\": 4.9247911862717856e-11, \"learning_rate\": 2.9390012642225034e-05, \"epoch\": 17.0, \"step\": 9299}\n",
      "\n",
      "预训练第16轮耗时： 60.10224986076355\n",
      "\n",
      "{\"loss\": 7.682206201665248e-11, \"learning_rate\": 3.11188369152971e-05, \"epoch\": 18.0, \"step\": 9846}\n",
      "\n",
      "预训练第17轮耗时： 60.26241493225098\n",
      "\n",
      "{\"loss\": 7.146459576864603e-11, \"learning_rate\": 3.284766118836916e-05, \"epoch\": 19.0, \"step\": 10393}\n",
      "\n",
      "预训练第18轮耗时： 59.96950078010559\n",
      "\n",
      "{\"loss\": 1.174250978800801e-10, \"learning_rate\": 3.457648546144122e-05, \"epoch\": 20.0, \"step\": 10940}\n",
      "\n",
      "预训练第19轮耗时： 60.22493577003479\n",
      "\n",
      "{\"loss\": 3.5357333960914404e-11, \"learning_rate\": 3.630530973451328e-05, \"epoch\": 21.0, \"step\": 11487}\n",
      "\n",
      "预训练第20轮耗时： 60.05142521858215\n",
      "\n",
      "{\"loss\": 1.0340690806171162e-10, \"learning_rate\": 3.803413400758534e-05, \"epoch\": 22.0, \"step\": 12034}\n",
      "\n",
      "预训练第21轮耗时： 60.0816855430603\n",
      "\n",
      "{\"loss\": 1.462369693143468e-10, \"learning_rate\": 3.97629582806574e-05, \"epoch\": 23.0, \"step\": 12581}\n",
      "\n",
      "预训练第22轮耗时： 60.24302792549133\n",
      "\n",
      "{\"loss\": 1.0335162342033329e-10, \"learning_rate\": 4.149178255372946e-05, \"epoch\": 24.0, \"step\": 13128}\n",
      "\n",
      "预训练第23轮耗时： 60.1010525226593\n",
      "\n",
      "{\"loss\": 1.533258216713842e-10, \"learning_rate\": 4.322060682680152e-05, \"epoch\": 25.0, \"step\": 13675}\n",
      "\n",
      "预训练第24轮耗时： 60.086690187454224\n",
      "\n",
      "{\"loss\": 2.4316045419321314e-10, \"learning_rate\": 4.494943109987358e-05, \"epoch\": 26.0, \"step\": 14222}\n",
      "\n",
      "预训练第25轮耗时： 60.088072061538696\n",
      "\n",
      "{\"loss\": 2.7569375098913323e-10, \"learning_rate\": 4.667825537294564e-05, \"epoch\": 27.0, \"step\": 14769}\n",
      "\n",
      "预训练第26轮耗时： 60.06942701339722\n",
      "\n",
      "{\"loss\": 1.213104546736401e-10, \"learning_rate\": 4.84070796460177e-05, \"epoch\": 28.0, \"step\": 15316}\n",
      "\n",
      "预训练第27轮耗时： 60.12759470939636\n",
      "\n",
      "{\"loss\": 1.7028937350213273e-10, \"learning_rate\": 4.999152625874471e-05, \"epoch\": 29.0, \"step\": 15863}\n",
      "\n",
      "预训练第28轮耗时： 60.04151010513306\n",
      "\n",
      "{\"loss\": 9.90603090110882e-11, \"learning_rate\": 4.988373238742733e-05, \"epoch\": 30.0, \"step\": 16410}\n",
      "\n",
      "预训练第29轮耗时： 59.867427349090576\n",
      "\n",
      "{\"loss\": 9.643834136707166e-11, \"learning_rate\": 4.9775938516109964e-05, \"epoch\": 31.0, \"step\": 16957}\n",
      "\n",
      "预训练第30轮耗时： 60.23043465614319\n",
      "\n",
      "{\"loss\": 4.84295017251707e-11, \"learning_rate\": 4.966814464479259e-05, \"epoch\": 32.0, \"step\": 17504}\n",
      "\n",
      "预训练第31轮耗时： 59.96008801460266\n",
      "\n",
      "{\"loss\": 8.382024548345942e-11, \"learning_rate\": 4.956035077347522e-05, \"epoch\": 33.0, \"step\": 18051}\n",
      "\n",
      "预训练第32轮耗时： 60.1547691822052\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.945255690215785e-05, \"epoch\": 34.0, \"step\": 18598}\n",
      "\n",
      "预训练第33轮耗时： 60.02582120895386\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.934476303084048e-05, \"epoch\": 35.0, \"step\": 19145}\n",
      "\n",
      "预训练第34轮耗时： 60.1325843334198\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.923696915952311e-05, \"epoch\": 36.0, \"step\": 19692}\n",
      "\n",
      "预训练第35轮耗时： 59.970457315444946\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.9129175288205734e-05, \"epoch\": 37.0, \"step\": 20239}\n",
      "\n",
      "预训练第36轮耗时： 60.11649680137634\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.9021381416888366e-05, \"epoch\": 38.0, \"step\": 20786}\n",
      "\n",
      "预训练第37轮耗时： 60.06378507614136\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.8913587545571e-05, \"epoch\": 39.0, \"step\": 21333}\n",
      "\n",
      "预训练第38轮耗时： 60.22858428955078\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.880579367425362e-05, \"epoch\": 40.0, \"step\": 21880}\n",
      "\n",
      "预训练第39轮耗时： 59.904048442840576\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.8697999802936254e-05, \"epoch\": 41.0, \"step\": 22427}\n",
      "\n",
      "预训练第40轮耗时： 60.11237406730652\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.859020593161888e-05, \"epoch\": 42.0, \"step\": 22974}\n",
      "\n",
      "预训练第41轮耗时： 60.070106983184814\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.848241206030151e-05, \"epoch\": 43.0, \"step\": 23521}\n",
      "\n",
      "预训练第42轮耗时： 60.31268501281738\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.837461818898414e-05, \"epoch\": 44.0, \"step\": 24068}\n",
      "\n",
      "预训练第43轮耗时： 60.19501781463623\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.826682431766677e-05, \"epoch\": 45.0, \"step\": 24615}\n",
      "\n",
      "预训练第44轮耗时： 59.97319221496582\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.81590304463494e-05, \"epoch\": 46.0, \"step\": 25162}\n",
      "\n",
      "预训练第45轮耗时： 60.125890254974365\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.805123657503202e-05, \"epoch\": 47.0, \"step\": 25709}\n",
      "\n",
      "预训练第46轮耗时： 60.09058165550232\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.7943442703714655e-05, \"epoch\": 48.0, \"step\": 26256}\n",
      "\n",
      "预训练第47轮耗时： 60.017603158950806\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.7835648832397286e-05, \"epoch\": 49.0, \"step\": 26803}\n",
      "\n",
      "预训练第48轮耗时： 60.004093408584595\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.772785496107991e-05, \"epoch\": 50.0, \"step\": 27350}\n",
      "\n",
      "预训练第49轮耗时： 60.25972270965576\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.762006108976254e-05, \"epoch\": 51.0, \"step\": 27897}\n",
      "\n",
      "预训练第50轮耗时： 60.200642108917236\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.751226721844517e-05, \"epoch\": 52.0, \"step\": 28444}\n",
      "\n",
      "预训练第51轮耗时： 60.111696004867554\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.74044733471278e-05, \"epoch\": 53.0, \"step\": 28991}\n",
      "\n",
      "预训练第52轮耗时： 60.006797313690186\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.729667947581043e-05, \"epoch\": 54.0, \"step\": 29538}\n",
      "\n",
      "预训练第53轮耗时： 60.28881096839905\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.7188885604493056e-05, \"epoch\": 55.0, \"step\": 30085}\n",
      "\n",
      "预训练第54轮耗时： 60.07877445220947\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.708109173317568e-05, \"epoch\": 56.0, \"step\": 30632}\n",
      "\n",
      "预训练第55轮耗时： 60.11437678337097\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.697329786185831e-05, \"epoch\": 57.0, \"step\": 31179}\n",
      "\n",
      "预训练第56轮耗时： 60.03195118904114\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.6865503990540944e-05, \"epoch\": 58.0, \"step\": 31726}\n",
      "\n",
      "预训练第57轮耗时： 60.11248993873596\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.675771011922357e-05, \"epoch\": 59.0, \"step\": 32273}\n",
      "\n",
      "预训练第58轮耗时： 60.04963040351868\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.66499162479062e-05, \"epoch\": 60.0, \"step\": 32820}\n",
      "\n",
      "预训练第59轮耗时： 60.07852530479431\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.6542122376588825e-05, \"epoch\": 61.0, \"step\": 33367}\n",
      "\n",
      "预训练第60轮耗时： 60.096925258636475\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.643432850527146e-05, \"epoch\": 62.0, \"step\": 33914}\n",
      "\n",
      "预训练第61轮耗时： 59.98648238182068\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.632653463395409e-05, \"epoch\": 63.0, \"step\": 34461}\n",
      "\n",
      "预训练第62轮耗时： 59.75758767127991\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.6218740762636713e-05, \"epoch\": 64.0, \"step\": 35008}\n",
      "\n",
      "预训练第63轮耗时： 60.27086639404297\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.6110946891319345e-05, \"epoch\": 65.0, \"step\": 35555}\n",
      "\n",
      "预训练第64轮耗时： 60.09006094932556\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.600315302000197e-05, \"epoch\": 66.0, \"step\": 36102}\n",
      "\n",
      "预训练第65轮耗时： 60.04171895980835\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.58953591486846e-05, \"epoch\": 67.0, \"step\": 36649}\n",
      "\n",
      "预训练第66轮耗时： 60.09902381896973\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.578756527736723e-05, \"epoch\": 68.0, \"step\": 37196}\n",
      "\n",
      "预训练第67轮耗时： 60.159281492233276\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.567977140604986e-05, \"epoch\": 69.0, \"step\": 37743}\n",
      "\n",
      "预训练第68轮耗时： 60.05729126930237\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.557197753473249e-05, \"epoch\": 70.0, \"step\": 38290}\n",
      "\n",
      "预训练第69轮耗时： 60.1638503074646\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.5464183663415114e-05, \"epoch\": 71.0, \"step\": 38837}\n",
      "\n",
      "预训练第70轮耗时： 59.96219992637634\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.5356389792097746e-05, \"epoch\": 72.0, \"step\": 39384}\n",
      "\n",
      "预训练第71轮耗时： 59.947251319885254\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.524859592078038e-05, \"epoch\": 73.0, \"step\": 39931}\n",
      "\n",
      "预训练第72轮耗时： 59.874229192733765\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.5140802049463e-05, \"epoch\": 74.0, \"step\": 40478}\n",
      "\n",
      "预训练第73轮耗时： 59.98803210258484\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.5033008178145634e-05, \"epoch\": 75.0, \"step\": 41025}\n",
      "\n",
      "预训练第74轮耗时： 60.028321266174316\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.492521430682826e-05, \"epoch\": 76.0, \"step\": 41572}\n",
      "\n",
      "预训练第75轮耗时： 60.0707745552063\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.481742043551089e-05, \"epoch\": 77.0, \"step\": 42119}\n",
      "\n",
      "预训练第76轮耗时： 60.122143507003784\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.470962656419352e-05, \"epoch\": 78.0, \"step\": 42666}\n",
      "\n",
      "预训练第77轮耗时： 59.69053053855896\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.460183269287615e-05, \"epoch\": 79.0, \"step\": 43213}\n",
      "\n",
      "预训练第78轮耗时： 60.04241394996643\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.449403882155878e-05, \"epoch\": 80.0, \"step\": 43760}\n",
      "\n",
      "预训练第79轮耗时： 60.18857717514038\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.4386244950241404e-05, \"epoch\": 81.0, \"step\": 44307}\n",
      "\n",
      "预训练第80轮耗时： 60.063541412353516\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.4278451078924035e-05, \"epoch\": 82.0, \"step\": 44854}\n",
      "\n",
      "预训练第81轮耗时： 59.98369479179382\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.417065720760667e-05, \"epoch\": 83.0, \"step\": 45401}\n",
      "\n",
      "预训练第82轮耗时： 59.89520025253296\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.406286333628929e-05, \"epoch\": 84.0, \"step\": 45948}\n",
      "\n",
      "预训练第83轮耗时： 59.88638162612915\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.3955069464971923e-05, \"epoch\": 85.0, \"step\": 46495}\n",
      "\n",
      "预训练第84轮耗时： 60.26124858856201\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.384727559365455e-05, \"epoch\": 86.0, \"step\": 47042}\n",
      "\n",
      "预训练第85轮耗时： 59.967058420181274\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.373948172233717e-05, \"epoch\": 87.0, \"step\": 47589}\n",
      "\n",
      "预训练第86轮耗时： 60.02146506309509\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.363168785101981e-05, \"epoch\": 88.0, \"step\": 48136}\n",
      "\n",
      "预训练第87轮耗时： 60.23112750053406\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.3523893979702436e-05, \"epoch\": 89.0, \"step\": 48683}\n",
      "\n",
      "预训练第88轮耗时： 59.98950481414795\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.341610010838507e-05, \"epoch\": 90.0, \"step\": 49230}\n",
      "\n",
      "预训练第89轮耗时： 60.0636146068573\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.330830623706769e-05, \"epoch\": 91.0, \"step\": 49777}\n",
      "\n",
      "预训练第90轮耗时： 59.96744465827942\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.320051236575032e-05, \"epoch\": 92.0, \"step\": 50324}\n",
      "\n",
      "预训练第91轮耗时： 60.136610984802246\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.3092718494432956e-05, \"epoch\": 93.0, \"step\": 50871}\n",
      "\n",
      "预训练第92轮耗时： 59.85032272338867\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.298492462311558e-05, \"epoch\": 94.0, \"step\": 51418}\n",
      "\n",
      "预训练第93轮耗时： 60.04088568687439\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.2877130751798206e-05, \"epoch\": 95.0, \"step\": 51965}\n",
      "\n",
      "预训练第94轮耗时： 60.00047969818115\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.276933688048084e-05, \"epoch\": 96.0, \"step\": 52512}\n",
      "\n",
      "预训练第95轮耗时： 60.247583866119385\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.266154300916346e-05, \"epoch\": 97.0, \"step\": 53059}\n",
      "\n",
      "预训练第96轮耗时： 59.869080781936646\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.2553749137846094e-05, \"epoch\": 98.0, \"step\": 53606}\n",
      "\n",
      "预训练第97轮耗时： 60.00777506828308\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.2445955266528726e-05, \"epoch\": 99.0, \"step\": 54153}\n",
      "\n",
      "预训练第98轮耗时： 60.07944893836975\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.233816139521135e-05, \"epoch\": 100.0, \"step\": 54700}\n",
      "\n",
      "预训练第99轮耗时： 60.18244552612305\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.223036752389398e-05, \"epoch\": 101.0, \"step\": 55247}\n",
      "\n",
      "预训练第100轮耗时： 60.02662992477417\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.212257365257661e-05, \"epoch\": 102.0, \"step\": 55794}\n",
      "\n",
      "预训练第101轮耗时： 59.96259355545044\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.201477978125924e-05, \"epoch\": 103.0, \"step\": 56341}\n",
      "\n",
      "预训练第102轮耗时： 60.082162380218506\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.190698590994187e-05, \"epoch\": 104.0, \"step\": 56888}\n",
      "\n",
      "预训练第103轮耗时： 59.83250689506531\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.1799192038624495e-05, \"epoch\": 105.0, \"step\": 57435}\n",
      "\n",
      "预训练第104轮耗时： 60.04027009010315\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.169139816730713e-05, \"epoch\": 106.0, \"step\": 57982}\n",
      "\n",
      "预训练第105轮耗时： 59.97821855545044\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.158360429598975e-05, \"epoch\": 107.0, \"step\": 58529}\n",
      "\n",
      "预训练第106轮耗时： 59.928122758865356\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.147581042467238e-05, \"epoch\": 108.0, \"step\": 59076}\n",
      "\n",
      "预训练第107轮耗时： 60.13288760185242\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.1368016553355015e-05, \"epoch\": 109.0, \"step\": 59623}\n",
      "\n",
      "预训练第108轮耗时： 60.012303590774536\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.126022268203764e-05, \"epoch\": 110.0, \"step\": 60170}\n",
      "\n",
      "预训练第109轮耗时： 59.99488878250122\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.115242881072027e-05, \"epoch\": 111.0, \"step\": 60717}\n",
      "\n",
      "预训练第110轮耗时： 60.21624541282654\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.1044634939402896e-05, \"epoch\": 112.0, \"step\": 61264}\n",
      "\n",
      "预训练第111轮耗时： 60.048643350601196\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.093684106808553e-05, \"epoch\": 113.0, \"step\": 61811}\n",
      "\n",
      "预训练第112轮耗时： 59.942890644073486\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.082904719676816e-05, \"epoch\": 114.0, \"step\": 62358}\n",
      "\n",
      "预训练第113轮耗时： 60.001686573028564\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.0721253325450784e-05, \"epoch\": 115.0, \"step\": 62905}\n",
      "\n",
      "预训练第114轮耗时： 59.874794244766235\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.0613459454133416e-05, \"epoch\": 116.0, \"step\": 63452}\n",
      "\n",
      "预训练第115轮耗时： 60.081244707107544\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.050566558281604e-05, \"epoch\": 117.0, \"step\": 63999}\n",
      "\n",
      "预训练第116轮耗时： 60.001394271850586\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.039787171149867e-05, \"epoch\": 118.0, \"step\": 64546}\n",
      "\n",
      "预训练第117轮耗时： 60.061057567596436\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.0290077840181304e-05, \"epoch\": 119.0, \"step\": 65093}\n",
      "\n",
      "预训练第118轮耗时： 60.18249273300171\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.018228396886393e-05, \"epoch\": 120.0, \"step\": 65640}\n",
      "\n",
      "预训练第119轮耗时： 60.04591941833496\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 4.007449009754656e-05, \"epoch\": 121.0, \"step\": 66187}\n",
      "\n",
      "预训练第120轮耗时： 59.961490631103516\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.9966696226229185e-05, \"epoch\": 122.0, \"step\": 66734}\n",
      "\n",
      "预训练第121轮耗时： 59.91145086288452\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.985890235491182e-05, \"epoch\": 123.0, \"step\": 67281}\n",
      "\n",
      "预训练第122轮耗时： 60.284400939941406\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.975110848359444e-05, \"epoch\": 124.0, \"step\": 67828}\n",
      "\n",
      "预训练第123轮耗时： 59.953903675079346\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.9643314612277073e-05, \"epoch\": 125.0, \"step\": 68375}\n",
      "\n",
      "预训练第124轮耗时： 59.960570096969604\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.9535520740959705e-05, \"epoch\": 126.0, \"step\": 68922}\n",
      "\n",
      "预训练第125轮耗时： 60.06721901893616\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.942772686964233e-05, \"epoch\": 127.0, \"step\": 69469}\n",
      "\n",
      "预训练第126轮耗时： 60.12930727005005\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.931993299832496e-05, \"epoch\": 128.0, \"step\": 70016}\n",
      "\n",
      "预训练第127轮耗时： 59.96990466117859\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.9212139127007586e-05, \"epoch\": 129.0, \"step\": 70563}\n",
      "\n",
      "预训练第128轮耗时： 60.06077790260315\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.910434525569022e-05, \"epoch\": 130.0, \"step\": 71110}\n",
      "\n",
      "预训练第129轮耗时： 59.94366478919983\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.899655138437285e-05, \"epoch\": 131.0, \"step\": 71657}\n",
      "\n",
      "预训练第130轮耗时： 60.245678424835205\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.8888757513055474e-05, \"epoch\": 132.0, \"step\": 72204}\n",
      "\n",
      "预训练第131轮耗时： 59.91993761062622\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.8780963641738106e-05, \"epoch\": 133.0, \"step\": 72751}\n",
      "\n",
      "预训练第132轮耗时： 60.046597719192505\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.867316977042073e-05, \"epoch\": 134.0, \"step\": 73298}\n",
      "\n",
      "预训练第133轮耗时： 60.22652554512024\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.856537589910336e-05, \"epoch\": 135.0, \"step\": 73845}\n",
      "\n",
      "预训练第134轮耗时： 60.01452350616455\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.8457582027785994e-05, \"epoch\": 136.0, \"step\": 74392}\n",
      "\n",
      "预训练第135轮耗时： 59.85423254966736\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.834978815646862e-05, \"epoch\": 137.0, \"step\": 74939}\n",
      "\n",
      "预训练第136轮耗时： 60.091182708740234\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.824199428515125e-05, \"epoch\": 138.0, \"step\": 75486}\n",
      "\n",
      "预训练第137轮耗时： 60.1276159286499\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.8134200413833876e-05, \"epoch\": 139.0, \"step\": 76033}\n",
      "\n",
      "预训练第138轮耗时： 60.01990532875061\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.802640654251651e-05, \"epoch\": 140.0, \"step\": 76580}\n",
      "\n",
      "预训练第139轮耗时： 60.03330731391907\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.791861267119913e-05, \"epoch\": 141.0, \"step\": 77127}\n",
      "\n",
      "预训练第140轮耗时： 60.009846687316895\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.7810818799881764e-05, \"epoch\": 142.0, \"step\": 77674}\n",
      "\n",
      "预训练第141轮耗时： 60.30274820327759\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.7703024928564395e-05, \"epoch\": 143.0, \"step\": 78221}\n",
      "\n",
      "预训练第142轮耗时： 60.24763107299805\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.759523105724702e-05, \"epoch\": 144.0, \"step\": 78768}\n",
      "\n",
      "预训练第143轮耗时： 60.06861329078674\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.748743718592965e-05, \"epoch\": 145.0, \"step\": 79315}\n",
      "\n",
      "预训练第144轮耗时： 59.967262983322144\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.737964331461228e-05, \"epoch\": 146.0, \"step\": 79862}\n",
      "\n",
      "预训练第145轮耗时： 59.80173325538635\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.727184944329491e-05, \"epoch\": 147.0, \"step\": 80409}\n",
      "\n",
      "预训练第146轮耗时： 60.23924732208252\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.716405557197754e-05, \"epoch\": 148.0, \"step\": 80956}\n",
      "\n",
      "预训练第147轮耗时： 60.09246110916138\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.7056261700660165e-05, \"epoch\": 149.0, \"step\": 81503}\n",
      "\n",
      "预训练第148轮耗时： 60.12312173843384\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.6948467829342796e-05, \"epoch\": 150.0, \"step\": 82050}\n",
      "\n",
      "预训练第149轮耗时： 59.99919056892395\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.684067395802542e-05, \"epoch\": 151.0, \"step\": 82597}\n",
      "\n",
      "预训练第150轮耗时： 60.206488609313965\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.673288008670805e-05, \"epoch\": 152.0, \"step\": 83144}\n",
      "\n",
      "预训练第151轮耗时： 60.03493785858154\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.6625086215390685e-05, \"epoch\": 153.0, \"step\": 83691}\n",
      "\n",
      "预训练第152轮耗时： 60.065202713012695\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.651729234407331e-05, \"epoch\": 154.0, \"step\": 84238}\n",
      "\n",
      "预训练第153轮耗时： 60.262547731399536\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.640949847275594e-05, \"epoch\": 155.0, \"step\": 84785}\n",
      "\n",
      "预训练第154轮耗时： 60.04835391044617\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.6301704601438566e-05, \"epoch\": 156.0, \"step\": 85332}\n",
      "\n",
      "预训练第155轮耗时： 60.0133752822876\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.61939107301212e-05, \"epoch\": 157.0, \"step\": 85879}\n",
      "\n",
      "预训练第156轮耗时： 59.92307949066162\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.608611685880383e-05, \"epoch\": 158.0, \"step\": 86426}\n",
      "\n",
      "预训练第157轮耗时： 60.17802691459656\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.5978322987486454e-05, \"epoch\": 159.0, \"step\": 86973}\n",
      "\n",
      "预训练第158轮耗时： 60.04374980926514\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.5870529116169086e-05, \"epoch\": 160.0, \"step\": 87520}\n",
      "\n",
      "预训练第159轮耗时： 60.09569215774536\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.576273524485171e-05, \"epoch\": 161.0, \"step\": 88067}\n",
      "\n",
      "预训练第160轮耗时： 60.020920753479004\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.565494137353434e-05, \"epoch\": 162.0, \"step\": 88614}\n",
      "\n",
      "预训练第161轮耗时： 60.19359469413757\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.5547147502216974e-05, \"epoch\": 163.0, \"step\": 89161}\n",
      "\n",
      "预训练第162轮耗时： 60.07736563682556\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.54393536308996e-05, \"epoch\": 164.0, \"step\": 89708}\n",
      "\n",
      "预训练第163轮耗时： 60.133504152297974\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.533155975958223e-05, \"epoch\": 165.0, \"step\": 90255}\n",
      "\n",
      "预训练第164轮耗时： 60.0024847984314\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.5223765888264855e-05, \"epoch\": 166.0, \"step\": 90802}\n",
      "\n",
      "预训练第165轮耗时： 60.19393706321716\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.511597201694749e-05, \"epoch\": 167.0, \"step\": 91349}\n",
      "\n",
      "预训练第166轮耗时： 60.03853464126587\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.500817814563012e-05, \"epoch\": 168.0, \"step\": 91896}\n",
      "\n",
      "预训练第167轮耗时： 59.90242123603821\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.490038427431274e-05, \"epoch\": 169.0, \"step\": 92443}\n",
      "\n",
      "预训练第168轮耗时： 60.16136145591736\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.4792590402995375e-05, \"epoch\": 170.0, \"step\": 92990}\n",
      "\n",
      "预训练第169轮耗时： 60.37791728973389\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.4684796531678e-05, \"epoch\": 171.0, \"step\": 93537}\n",
      "\n",
      "预训练第170轮耗时： 60.049288272857666\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.457700266036063e-05, \"epoch\": 172.0, \"step\": 94084}\n",
      "\n",
      "预训练第171轮耗时： 60.22886872291565\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.446920878904326e-05, \"epoch\": 173.0, \"step\": 94631}\n",
      "\n",
      "预训练第172轮耗时： 60.305771589279175\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.436141491772589e-05, \"epoch\": 174.0, \"step\": 95178}\n",
      "\n",
      "预训练第173轮耗时： 60.019933462142944\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.425362104640852e-05, \"epoch\": 175.0, \"step\": 95725}\n",
      "\n",
      "预训练第174轮耗时： 60.143723011016846\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.4145827175091144e-05, \"epoch\": 176.0, \"step\": 96272}\n",
      "\n",
      "预训练第175轮耗时： 60.16867923736572\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.403803330377377e-05, \"epoch\": 177.0, \"step\": 96819}\n",
      "\n",
      "预训练第176轮耗时： 60.28424429893494\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.393023943245641e-05, \"epoch\": 178.0, \"step\": 97366}\n",
      "\n",
      "预训练第177轮耗时： 60.036081075668335\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.382244556113903e-05, \"epoch\": 179.0, \"step\": 97913}\n",
      "\n",
      "预训练第178轮耗时： 60.009172201156616\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.371465168982166e-05, \"epoch\": 180.0, \"step\": 98460}\n",
      "\n",
      "预训练第179轮耗时： 60.0899932384491\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.360685781850429e-05, \"epoch\": 181.0, \"step\": 99007}\n",
      "\n",
      "预训练第180轮耗时： 60.238998889923096\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.3499063947186914e-05, \"epoch\": 182.0, \"step\": 99554}\n",
      "\n",
      "预训练第181轮耗时： 59.99243760108948\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.3391270075869545e-05, \"epoch\": 183.0, \"step\": 100101}\n",
      "\n",
      "预训练第182轮耗时： 60.29519009590149\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.328347620455218e-05, \"epoch\": 184.0, \"step\": 100648}\n",
      "\n",
      "预训练第183轮耗时： 60.07571625709534\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.31756823332348e-05, \"epoch\": 185.0, \"step\": 101195}\n",
      "\n",
      "预训练第184轮耗时： 60.11996293067932\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.3067888461917433e-05, \"epoch\": 186.0, \"step\": 101742}\n",
      "\n",
      "预训练第185轮耗时： 60.20006990432739\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.296009459060006e-05, \"epoch\": 187.0, \"step\": 102289}\n",
      "\n",
      "预训练第186轮耗时： 60.03985667228699\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.285230071928269e-05, \"epoch\": 188.0, \"step\": 102836}\n",
      "\n",
      "预训练第187轮耗时： 60.004615783691406\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.274450684796532e-05, \"epoch\": 189.0, \"step\": 103383}\n",
      "\n",
      "预训练第188轮耗时： 60.112709283828735\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.2636712976647946e-05, \"epoch\": 190.0, \"step\": 103930}\n",
      "\n",
      "预训练第189轮耗时： 59.906235694885254\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.252891910533058e-05, \"epoch\": 191.0, \"step\": 104477}\n",
      "\n",
      "预训练第190轮耗时： 59.92883610725403\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.24211252340132e-05, \"epoch\": 192.0, \"step\": 105024}\n",
      "\n",
      "预训练第191轮耗时： 60.0675253868103\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.2313331362695835e-05, \"epoch\": 193.0, \"step\": 105571}\n",
      "\n",
      "预训练第192轮耗时： 60.28461694717407\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.2205537491378466e-05, \"epoch\": 194.0, \"step\": 106118}\n",
      "\n",
      "预训练第193轮耗时： 60.107301235198975\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.209774362006109e-05, \"epoch\": 195.0, \"step\": 106665}\n",
      "\n",
      "预训练第194轮耗时： 59.94700884819031\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.198994974874372e-05, \"epoch\": 196.0, \"step\": 107212}\n",
      "\n",
      "预训练第195轮耗时： 60.150710105895996\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.188215587742635e-05, \"epoch\": 197.0, \"step\": 107759}\n",
      "\n",
      "预训练第196轮耗时： 60.06159806251526\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.177436200610898e-05, \"epoch\": 198.0, \"step\": 108306}\n",
      "\n",
      "预训练第197轮耗时： 60.06058478355408\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.166656813479161e-05, \"epoch\": 199.0, \"step\": 108853}\n",
      "\n",
      "预训练第198轮耗时： 59.9931640625\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.1558774263474236e-05, \"epoch\": 200.0, \"step\": 109400}\n",
      "\n",
      "预训练第199轮耗时： 59.97574520111084\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.145098039215687e-05, \"epoch\": 201.0, \"step\": 109947}\n",
      "\n",
      "预训练第200轮耗时： 60.24147152900696\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.134318652083949e-05, \"epoch\": 202.0, \"step\": 110494}\n",
      "\n",
      "预训练第201轮耗时： 59.95452094078064\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.1235392649522124e-05, \"epoch\": 203.0, \"step\": 111041}\n",
      "\n",
      "预训练第202轮耗时： 60.050339221954346\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.1127598778204755e-05, \"epoch\": 204.0, \"step\": 111588}\n",
      "\n",
      "预训练第203轮耗时： 60.125004529953\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.101980490688738e-05, \"epoch\": 205.0, \"step\": 112135}\n",
      "\n",
      "预训练第204轮耗时： 59.924049377441406\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.091201103557001e-05, \"epoch\": 206.0, \"step\": 112682}\n",
      "\n",
      "预训练第205轮耗时： 59.893474817276\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.080421716425264e-05, \"epoch\": 207.0, \"step\": 113229}\n",
      "\n",
      "预训练第206轮耗时： 59.98324656486511\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.069642329293527e-05, \"epoch\": 208.0, \"step\": 113776}\n",
      "\n",
      "预训练第207轮耗时： 59.976195335388184\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.05886294216179e-05, \"epoch\": 209.0, \"step\": 114323}\n",
      "\n",
      "预训练第208轮耗时： 59.98492479324341\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.0480835550300525e-05, \"epoch\": 210.0, \"step\": 114870}\n",
      "\n",
      "预训练第209轮耗时： 59.939072132110596\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.037304167898315e-05, \"epoch\": 211.0, \"step\": 115417}\n",
      "\n",
      "预训练第210轮耗时： 59.846850633621216\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.026524780766578e-05, \"epoch\": 212.0, \"step\": 115964}\n",
      "\n",
      "预训练第211轮耗时： 60.17838788032532\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.0157453936348413e-05, \"epoch\": 213.0, \"step\": 116511}\n",
      "\n",
      "预训练第212轮耗时： 59.965670585632324\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 3.0049660065031038e-05, \"epoch\": 214.0, \"step\": 117058}\n",
      "\n",
      "预训练第213轮耗时： 60.029109477996826\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.994186619371367e-05, \"epoch\": 215.0, \"step\": 117605}\n",
      "\n",
      "预训练第214轮耗时： 59.980844497680664\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.9834072322396294e-05, \"epoch\": 216.0, \"step\": 118152}\n",
      "\n",
      "预训练第215轮耗时： 60.07436275482178\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.9726278451078926e-05, \"epoch\": 217.0, \"step\": 118699}\n",
      "\n",
      "预训练第216轮耗时： 59.940901041030884\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.9618484579761557e-05, \"epoch\": 218.0, \"step\": 119246}\n",
      "\n",
      "预训练第217轮耗时： 59.953221559524536\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.9510690708444182e-05, \"epoch\": 219.0, \"step\": 119793}\n",
      "\n",
      "预训练第218轮耗时： 60.24827480316162\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.9402896837126814e-05, \"epoch\": 220.0, \"step\": 120340}\n",
      "\n",
      "预训练第219轮耗时： 60.00761389732361\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.929510296580944e-05, \"epoch\": 221.0, \"step\": 120887}\n",
      "\n",
      "预训练第220轮耗时： 59.93866682052612\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.918730909449207e-05, \"epoch\": 222.0, \"step\": 121434}\n",
      "\n",
      "预训练第221轮耗时： 60.21700072288513\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.9079515223174702e-05, \"epoch\": 223.0, \"step\": 121981}\n",
      "\n",
      "预训练第222轮耗时： 60.383753061294556\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.8971721351857327e-05, \"epoch\": 224.0, \"step\": 122528}\n",
      "\n",
      "预训练第223轮耗时： 60.42008090019226\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.886392748053996e-05, \"epoch\": 225.0, \"step\": 123075}\n",
      "\n",
      "预训练第224轮耗时： 60.160724401474\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.8756133609222587e-05, \"epoch\": 226.0, \"step\": 123622}\n",
      "\n",
      "预训练第225轮耗时： 60.12543249130249\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.8648339737905215e-05, \"epoch\": 227.0, \"step\": 124169}\n",
      "\n",
      "预训练第226轮耗时： 60.18459606170654\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.8540545866587847e-05, \"epoch\": 228.0, \"step\": 124716}\n",
      "\n",
      "预训练第227轮耗时： 59.927983045578\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.8432751995270475e-05, \"epoch\": 229.0, \"step\": 125263}\n",
      "\n",
      "预训练第228轮耗时： 60.04216766357422\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.8324958123953103e-05, \"epoch\": 230.0, \"step\": 125810}\n",
      "\n",
      "预训练第229轮耗时： 60.07606220245361\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.821716425263573e-05, \"epoch\": 231.0, \"step\": 126357}\n",
      "\n",
      "预训练第230轮耗时： 60.858131885528564\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.8109370381318363e-05, \"epoch\": 232.0, \"step\": 126904}\n",
      "\n",
      "预训练第231轮耗时： 61.24934720993042\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.8001576510000995e-05, \"epoch\": 233.0, \"step\": 127451}\n",
      "\n",
      "预训练第232轮耗时： 61.09395408630371\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.789378263868362e-05, \"epoch\": 234.0, \"step\": 127998}\n",
      "\n",
      "预训练第233轮耗时： 61.23923945426941\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.778598876736625e-05, \"epoch\": 235.0, \"step\": 128545}\n",
      "\n",
      "预训练第234轮耗时： 61.40500497817993\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.767819489604887e-05, \"epoch\": 236.0, \"step\": 129092}\n",
      "\n",
      "预训练第235轮耗时： 61.338555335998535\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.7570401024731508e-05, \"epoch\": 237.0, \"step\": 129639}\n",
      "\n",
      "预训练第236轮耗时： 61.23843812942505\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.746260715341414e-05, \"epoch\": 238.0, \"step\": 130186}\n",
      "\n",
      "预训练第237轮耗时： 61.318448066711426\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.7354813282096757e-05, \"epoch\": 239.0, \"step\": 130733}\n",
      "\n",
      "预训练第238轮耗时： 61.394445180892944\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.724701941077939e-05, \"epoch\": 240.0, \"step\": 131280}\n",
      "\n",
      "预训练第239轮耗时： 61.274070501327515\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.7139225539462014e-05, \"epoch\": 241.0, \"step\": 131827}\n",
      "\n",
      "预训练第240轮耗时： 61.22999143600464\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.7031431668144642e-05, \"epoch\": 242.0, \"step\": 132374}\n",
      "\n",
      "预训练第241轮耗时： 61.28079128265381\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.6923637796827277e-05, \"epoch\": 243.0, \"step\": 132921}\n",
      "\n",
      "预训练第242轮耗时： 61.29204320907593\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.6815843925509905e-05, \"epoch\": 244.0, \"step\": 133468}\n",
      "\n",
      "预训练第243轮耗时： 61.13503384590149\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.6708050054192534e-05, \"epoch\": 245.0, \"step\": 134015}\n",
      "\n",
      "预训练第244轮耗时： 61.03308653831482\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.6600256182875162e-05, \"epoch\": 246.0, \"step\": 134562}\n",
      "\n",
      "预训练第245轮耗时： 61.053378105163574\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.6492462311557787e-05, \"epoch\": 247.0, \"step\": 135109}\n",
      "\n",
      "预训练第246轮耗时： 60.96500277519226\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.638466844024042e-05, \"epoch\": 248.0, \"step\": 135656}\n",
      "\n",
      "预训练第247轮耗时： 60.8750696182251\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.627687456892305e-05, \"epoch\": 249.0, \"step\": 136203}\n",
      "\n",
      "预训练第248轮耗时： 60.96683406829834\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.6169080697605675e-05, \"epoch\": 250.0, \"step\": 136750}\n",
      "\n",
      "预训练第249轮耗时： 60.838268995285034\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.6061286826288306e-05, \"epoch\": 251.0, \"step\": 137297}\n",
      "\n",
      "预训练第250轮耗时： 61.08226799964905\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.595349295497093e-05, \"epoch\": 252.0, \"step\": 137844}\n",
      "\n",
      "预训练第251轮耗时： 60.439892292022705\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.5845699083653563e-05, \"epoch\": 253.0, \"step\": 138391}\n",
      "\n",
      "预训练第252轮耗时： 60.02916383743286\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.5737905212336195e-05, \"epoch\": 254.0, \"step\": 138938}\n",
      "\n",
      "预训练第253轮耗时： 60.077595233917236\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.563011134101882e-05, \"epoch\": 255.0, \"step\": 139485}\n",
      "\n",
      "预训练第254轮耗时： 60.17523813247681\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.552231746970145e-05, \"epoch\": 256.0, \"step\": 140032}\n",
      "\n",
      "预训练第255轮耗时： 60.037373781204224\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.5414523598384076e-05, \"epoch\": 257.0, \"step\": 140579}\n",
      "\n",
      "预训练第256轮耗时： 60.11219906806946\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.5306729727066707e-05, \"epoch\": 258.0, \"step\": 141126}\n",
      "\n",
      "预训练第257轮耗时： 60.30924153327942\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.519893585574934e-05, \"epoch\": 259.0, \"step\": 141673}\n",
      "\n",
      "预训练第258轮耗时： 60.087379693984985\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.5091141984431964e-05, \"epoch\": 260.0, \"step\": 142220}\n",
      "\n",
      "预训练第259轮耗时： 60.15121674537659\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.4983348113114596e-05, \"epoch\": 261.0, \"step\": 142767}\n",
      "\n",
      "预训练第260轮耗时： 60.135440826416016\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.487555424179722e-05, \"epoch\": 262.0, \"step\": 143314}\n",
      "\n",
      "预训练第261轮耗时： 59.976858377456665\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.4767760370479852e-05, \"epoch\": 263.0, \"step\": 143861}\n",
      "\n",
      "预训练第262轮耗时： 60.02329421043396\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.4659966499162484e-05, \"epoch\": 264.0, \"step\": 144408}\n",
      "\n",
      "预训练第263轮耗时： 60.27435660362244\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.455217262784511e-05, \"epoch\": 265.0, \"step\": 144955}\n",
      "\n",
      "预训练第264轮耗时： 60.377999782562256\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.444437875652774e-05, \"epoch\": 266.0, \"step\": 145502}\n",
      "\n",
      "预训练第265轮耗时： 60.26639127731323\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.4336584885210365e-05, \"epoch\": 267.0, \"step\": 146049}\n",
      "\n",
      "预训练第266轮耗时： 60.00276207923889\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.4228791013892997e-05, \"epoch\": 268.0, \"step\": 146596}\n",
      "\n",
      "预训练第267轮耗时： 60.1630859375\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.412099714257563e-05, \"epoch\": 269.0, \"step\": 147143}\n",
      "\n",
      "预训练第268轮耗时： 60.138620376586914\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.4013203271258253e-05, \"epoch\": 270.0, \"step\": 147690}\n",
      "\n",
      "预训练第269轮耗时： 60.22096037864685\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.390540939994088e-05, \"epoch\": 271.0, \"step\": 148237}\n",
      "\n",
      "预训练第270轮耗时： 60.11305093765259\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.379761552862351e-05, \"epoch\": 272.0, \"step\": 148784}\n",
      "\n",
      "预训练第271轮耗时： 60.13794422149658\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.368982165730614e-05, \"epoch\": 273.0, \"step\": 149331}\n",
      "\n",
      "预训练第272轮耗时： 60.229450702667236\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.358202778598877e-05, \"epoch\": 274.0, \"step\": 149878}\n",
      "\n",
      "预训练第273轮耗时： 60.14795994758606\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.3474233914671398e-05, \"epoch\": 275.0, \"step\": 150425}\n",
      "\n",
      "预训练第274轮耗时： 59.98737382888794\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.3366440043354026e-05, \"epoch\": 276.0, \"step\": 150972}\n",
      "\n",
      "预训练第275轮耗时： 60.06453013420105\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.3258646172036654e-05, \"epoch\": 277.0, \"step\": 151519}\n",
      "\n",
      "预训练第276轮耗时： 60.321096897125244\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.3150852300719286e-05, \"epoch\": 278.0, \"step\": 152066}\n",
      "\n",
      "预训练第277轮耗时： 60.78669238090515\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.3043058429401914e-05, \"epoch\": 279.0, \"step\": 152613}\n",
      "\n",
      "预训练第278轮耗时： 60.727479219436646\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.2935264558084542e-05, \"epoch\": 280.0, \"step\": 153160}\n",
      "\n",
      "预训练第279轮耗时： 60.265815019607544\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.282747068676717e-05, \"epoch\": 281.0, \"step\": 153707}\n",
      "\n",
      "预训练第280轮耗时： 60.18750882148743\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.27196768154498e-05, \"epoch\": 282.0, \"step\": 154254}\n",
      "\n",
      "预训练第281轮耗时： 60.4172465801239\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.261188294413243e-05, \"epoch\": 283.0, \"step\": 154801}\n",
      "\n",
      "预训练第282轮耗时： 59.954561710357666\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.250408907281506e-05, \"epoch\": 284.0, \"step\": 155348}\n",
      "\n",
      "预训练第283轮耗时： 60.092233657836914\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.2396295201497687e-05, \"epoch\": 285.0, \"step\": 155895}\n",
      "\n",
      "预训练第284轮耗时： 60.02918004989624\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.2288501330180315e-05, \"epoch\": 286.0, \"step\": 156442}\n",
      "\n",
      "预训练第285轮耗时： 60.19570350646973\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.2180707458862943e-05, \"epoch\": 287.0, \"step\": 156989}\n",
      "\n",
      "预训练第286轮耗时： 60.02301526069641\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.2072913587545575e-05, \"epoch\": 288.0, \"step\": 157536}\n",
      "\n",
      "预训练第287轮耗时： 59.895198583602905\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.1965119716228203e-05, \"epoch\": 289.0, \"step\": 158083}\n",
      "\n",
      "预训练第288轮耗时： 60.036558866500854\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.185732584491083e-05, \"epoch\": 290.0, \"step\": 158630}\n",
      "\n",
      "预训练第289轮耗时： 60.09541630744934\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.1749531973593456e-05, \"epoch\": 291.0, \"step\": 159177}\n",
      "\n",
      "预训练第290轮耗时： 60.04524540901184\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.1641738102276088e-05, \"epoch\": 292.0, \"step\": 159724}\n",
      "\n",
      "预训练第291轮耗时： 60.00901222229004\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.153394423095872e-05, \"epoch\": 293.0, \"step\": 160271}\n",
      "\n",
      "预训练第292轮耗时： 60.18269681930542\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.1426150359641345e-05, \"epoch\": 294.0, \"step\": 160818}\n",
      "\n",
      "预训练第293轮耗时： 59.90637803077698\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.1318356488323976e-05, \"epoch\": 295.0, \"step\": 161365}\n",
      "\n",
      "预训练第294轮耗时： 60.1712384223938\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.12105626170066e-05, \"epoch\": 296.0, \"step\": 161912}\n",
      "\n",
      "预训练第295轮耗时： 60.016396045684814\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.1102768745689233e-05, \"epoch\": 297.0, \"step\": 162459}\n",
      "\n",
      "预训练第296轮耗时： 60.05617094039917\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.0994974874371864e-05, \"epoch\": 298.0, \"step\": 163006}\n",
      "\n",
      "预训练第297轮耗时： 60.03675436973572\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.088718100305449e-05, \"epoch\": 299.0, \"step\": 163553}\n",
      "\n",
      "预训练第298轮耗时： 60.01119804382324\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.077938713173712e-05, \"epoch\": 300.0, \"step\": 164100}\n",
      "\n",
      "预训练第299轮耗时： 60.16181778907776\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.0671593260419746e-05, \"epoch\": 301.0, \"step\": 164647}\n",
      "\n",
      "预训练第300轮耗时： 60.353694915771484\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.0563799389102374e-05, \"epoch\": 302.0, \"step\": 165194}\n",
      "\n",
      "预训练第301轮耗时： 60.15473437309265\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.045600551778501e-05, \"epoch\": 303.0, \"step\": 165741}\n",
      "\n",
      "预训练第302轮耗时： 60.042762756347656\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.0348211646467634e-05, \"epoch\": 304.0, \"step\": 166288}\n",
      "\n",
      "预训练第303轮耗时： 60.022966146469116\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.0240417775150265e-05, \"epoch\": 305.0, \"step\": 166835}\n",
      "\n",
      "预训练第304轮耗时： 60.15227961540222\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.013262390383289e-05, \"epoch\": 306.0, \"step\": 167382}\n",
      "\n",
      "预训练第305轮耗时： 60.05781149864197\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 2.002483003251552e-05, \"epoch\": 307.0, \"step\": 167929}\n",
      "\n",
      "预训练第306轮耗时： 60.04899191856384\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.9917036161198153e-05, \"epoch\": 308.0, \"step\": 168476}\n",
      "\n",
      "预训练第307轮耗时： 59.98472738265991\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.980924228988078e-05, \"epoch\": 309.0, \"step\": 169023}\n",
      "\n",
      "预训练第308轮耗时： 60.01443600654602\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.9701448418563407e-05, \"epoch\": 310.0, \"step\": 169570}\n",
      "\n",
      "预训练第309轮耗时： 59.98123621940613\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.9593654547246035e-05, \"epoch\": 311.0, \"step\": 170117}\n",
      "\n",
      "预训练第310轮耗时： 60.00784397125244\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.9485860675928663e-05, \"epoch\": 312.0, \"step\": 170664}\n",
      "\n",
      "预训练第311轮耗时： 60.01961708068848\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.9378066804611295e-05, \"epoch\": 313.0, \"step\": 171211}\n",
      "\n",
      "预训练第312轮耗时： 60.24971580505371\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.9270272933293923e-05, \"epoch\": 314.0, \"step\": 171758}\n",
      "\n",
      "预训练第313轮耗时： 59.97908663749695\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.916247906197655e-05, \"epoch\": 315.0, \"step\": 172305}\n",
      "\n",
      "预训练第314轮耗时： 59.97590208053589\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.905468519065918e-05, \"epoch\": 316.0, \"step\": 172852}\n",
      "\n",
      "预训练第315轮耗时： 60.03164339065552\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.8946891319341808e-05, \"epoch\": 317.0, \"step\": 173399}\n",
      "\n",
      "预训练第316轮耗时： 60.14354228973389\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.883909744802444e-05, \"epoch\": 318.0, \"step\": 173946}\n",
      "\n",
      "预训练第317轮耗时： 60.008405923843384\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.8731303576707068e-05, \"epoch\": 319.0, \"step\": 174493}\n",
      "\n",
      "预训练第318轮耗时： 60.0373330116272\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.8623509705389696e-05, \"epoch\": 320.0, \"step\": 175040}\n",
      "\n",
      "预训练第319轮耗时： 60.17652606964111\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.8515715834072324e-05, \"epoch\": 321.0, \"step\": 175587}\n",
      "\n",
      "预训练第320轮耗时： 60.25411868095398\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.840792196275495e-05, \"epoch\": 322.0, \"step\": 176134}\n",
      "\n",
      "预训练第321轮耗时： 60.02363348007202\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.8300128091437584e-05, \"epoch\": 323.0, \"step\": 176681}\n",
      "\n",
      "预训练第322轮耗时： 60.03911018371582\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.8192334220120212e-05, \"epoch\": 324.0, \"step\": 177228}\n",
      "\n",
      "预训练第323轮耗时： 60.068023681640625\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.8084540348802837e-05, \"epoch\": 325.0, \"step\": 177775}\n",
      "\n",
      "预训练第324轮耗时： 60.013678789138794\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.797674647748547e-05, \"epoch\": 326.0, \"step\": 178322}\n",
      "\n",
      "预训练第325轮耗时： 60.050371408462524\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.7868952606168093e-05, \"epoch\": 327.0, \"step\": 178869}\n",
      "\n",
      "预训练第326轮耗时： 60.08448910713196\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.7761158734850725e-05, \"epoch\": 328.0, \"step\": 179416}\n",
      "\n",
      "预训练第327轮耗时： 60.514230489730835\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.7653364863533357e-05, \"epoch\": 329.0, \"step\": 179963}\n",
      "\n",
      "预训练第328轮耗时： 60.319236516952515\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.754557099221598e-05, \"epoch\": 330.0, \"step\": 180510}\n",
      "\n",
      "预训练第329轮耗时： 60.324453830718994\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.7437777120898613e-05, \"epoch\": 331.0, \"step\": 181057}\n",
      "\n",
      "预训练第330轮耗时： 60.29705309867859\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.7329983249581238e-05, \"epoch\": 332.0, \"step\": 181604}\n",
      "\n",
      "预训练第331轮耗时： 60.600679874420166\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.722218937826387e-05, \"epoch\": 333.0, \"step\": 182151}\n",
      "\n",
      "预训练第332轮耗时： 60.51364541053772\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.71143955069465e-05, \"epoch\": 334.0, \"step\": 182698}\n",
      "\n",
      "预训练第333轮耗时： 60.56918144226074\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.7006601635629126e-05, \"epoch\": 335.0, \"step\": 183245}\n",
      "\n",
      "预训练第334轮耗时： 60.52722120285034\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.6898807764311754e-05, \"epoch\": 336.0, \"step\": 183792}\n",
      "\n",
      "预训练第335轮耗时： 60.725454807281494\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.6791013892994383e-05, \"epoch\": 337.0, \"step\": 184339}\n",
      "\n",
      "预训练第336轮耗时： 60.24039363861084\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.6683220021677014e-05, \"epoch\": 338.0, \"step\": 184886}\n",
      "\n",
      "预训练第337轮耗时： 60.28691387176514\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.6575426150359642e-05, \"epoch\": 339.0, \"step\": 185433}\n",
      "\n",
      "预训练第338轮耗时： 60.2504997253418\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.646763227904227e-05, \"epoch\": 340.0, \"step\": 185980}\n",
      "\n",
      "预训练第339轮耗时： 60.41024398803711\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.63598384077249e-05, \"epoch\": 341.0, \"step\": 186527}\n",
      "\n",
      "预训练第340轮耗时： 60.32429909706116\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.6252044536407527e-05, \"epoch\": 342.0, \"step\": 187074}\n",
      "\n",
      "预训练第341轮耗时： 60.312695264816284\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.614425066509016e-05, \"epoch\": 343.0, \"step\": 187621}\n",
      "\n",
      "预训练第342轮耗时： 60.26982045173645\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.6036456793772787e-05, \"epoch\": 344.0, \"step\": 188168}\n",
      "\n",
      "预训练第343轮耗时： 60.66087079048157\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.5928662922455415e-05, \"epoch\": 345.0, \"step\": 188715}\n",
      "\n",
      "预训练第344轮耗时： 60.30732583999634\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.5820869051138044e-05, \"epoch\": 346.0, \"step\": 189262}\n",
      "\n",
      "预训练第345轮耗时： 60.374292850494385\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.5713075179820672e-05, \"epoch\": 347.0, \"step\": 189809}\n",
      "\n",
      "预训练第346轮耗时： 60.464564085006714\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.5605281308503303e-05, \"epoch\": 348.0, \"step\": 190356}\n",
      "\n",
      "预训练第347轮耗时： 60.35249376296997\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.549748743718593e-05, \"epoch\": 349.0, \"step\": 190903}\n",
      "\n",
      "预训练第348轮耗时： 60.19617486000061\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.538969356586856e-05, \"epoch\": 350.0, \"step\": 191450}\n",
      "\n",
      "预训练第349轮耗时： 60.24949097633362\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.5281899694551188e-05, \"epoch\": 351.0, \"step\": 191997}\n",
      "\n",
      "预训练第350轮耗时： 60.3692946434021\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.5174105823233816e-05, \"epoch\": 352.0, \"step\": 192544}\n",
      "\n",
      "预训练第351轮耗时： 60.3320631980896\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.5066311951916448e-05, \"epoch\": 353.0, \"step\": 193091}\n",
      "\n",
      "预训练第352轮耗时： 60.27464294433594\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.4958518080599076e-05, \"epoch\": 354.0, \"step\": 193638}\n",
      "\n",
      "预训练第353轮耗时： 60.23129343986511\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.4850724209281705e-05, \"epoch\": 355.0, \"step\": 194185}\n",
      "\n",
      "预训练第354轮耗时： 60.60259962081909\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.4742930337964333e-05, \"epoch\": 356.0, \"step\": 194732}\n",
      "\n",
      "预训练第355轮耗时： 60.26163578033447\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.4635136466646963e-05, \"epoch\": 357.0, \"step\": 195279}\n",
      "\n",
      "预训练第356轮耗时： 60.3690664768219\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.4527342595329593e-05, \"epoch\": 358.0, \"step\": 195826}\n",
      "\n",
      "预训练第357轮耗时： 60.326329469680786\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.4419548724012223e-05, \"epoch\": 359.0, \"step\": 196373}\n",
      "\n",
      "预训练第358轮耗时： 60.41187405586243\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.431175485269485e-05, \"epoch\": 360.0, \"step\": 196920}\n",
      "\n",
      "预训练第359轮耗时： 60.251425981521606\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.4203960981377476e-05, \"epoch\": 361.0, \"step\": 197467}\n",
      "\n",
      "预训练第360轮耗时： 60.3204185962677\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.4096167110060104e-05, \"epoch\": 362.0, \"step\": 198014}\n",
      "\n",
      "预训练第361轮耗时： 60.22266364097595\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.3988373238742739e-05, \"epoch\": 363.0, \"step\": 198561}\n",
      "\n",
      "预训练第362轮耗时： 60.46747875213623\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.3880579367425364e-05, \"epoch\": 364.0, \"step\": 199108}\n",
      "\n",
      "预训练第363轮耗时： 60.318410873413086\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.3772785496107992e-05, \"epoch\": 365.0, \"step\": 199655}\n",
      "\n",
      "预训练第364轮耗时： 60.37823963165283\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.3664991624790622e-05, \"epoch\": 366.0, \"step\": 200202}\n",
      "\n",
      "预训练第365轮耗时： 60.28421401977539\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.355719775347325e-05, \"epoch\": 367.0, \"step\": 200749}\n",
      "\n",
      "预训练第366轮耗时： 60.59866261482239\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.344940388215588e-05, \"epoch\": 368.0, \"step\": 201296}\n",
      "\n",
      "预训练第367轮耗时： 60.426557302474976\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.334161001083851e-05, \"epoch\": 369.0, \"step\": 201843}\n",
      "\n",
      "预训练第368轮耗时： 60.42551612854004\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.3233816139521138e-05, \"epoch\": 370.0, \"step\": 202390}\n",
      "\n",
      "预训练第369轮耗时： 60.28877091407776\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.3126022268203763e-05, \"epoch\": 371.0, \"step\": 202937}\n",
      "\n",
      "预训练第370轮耗时： 60.30344581604004\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.3018228396886391e-05, \"epoch\": 372.0, \"step\": 203484}\n",
      "\n",
      "预训练第371轮耗时： 60.27519416809082\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.2910434525569023e-05, \"epoch\": 373.0, \"step\": 204031}\n",
      "\n",
      "预训练第372轮耗时： 60.32480192184448\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.2802640654251651e-05, \"epoch\": 374.0, \"step\": 204578}\n",
      "\n",
      "预训练第373轮耗时： 60.218427658081055\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.2694846782934281e-05, \"epoch\": 375.0, \"step\": 205125}\n",
      "\n",
      "预训练第374轮耗时： 60.304529428482056\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.258705291161691e-05, \"epoch\": 376.0, \"step\": 205672}\n",
      "\n",
      "预训练第375轮耗时： 60.20366168022156\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.2479259040299536e-05, \"epoch\": 377.0, \"step\": 206219}\n",
      "\n",
      "预训练第376轮耗时： 60.29580020904541\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.2371465168982168e-05, \"epoch\": 378.0, \"step\": 206766}\n",
      "\n",
      "预训练第377轮耗时： 60.39051795005798\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.2263671297664796e-05, \"epoch\": 379.0, \"step\": 207313}\n",
      "\n",
      "预训练第378轮耗时： 60.3455011844635\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.2155877426347424e-05, \"epoch\": 380.0, \"step\": 207860}\n",
      "\n",
      "预训练第379轮耗时： 60.256574869155884\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.2048083555030052e-05, \"epoch\": 381.0, \"step\": 208407}\n",
      "\n",
      "预训练第380轮耗时： 60.53710746765137\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.194028968371268e-05, \"epoch\": 382.0, \"step\": 208954}\n",
      "\n",
      "预训练第381轮耗时： 60.56235909461975\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.1832495812395312e-05, \"epoch\": 383.0, \"step\": 209501}\n",
      "\n",
      "预训练第382轮耗时： 60.437636852264404\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.172470194107794e-05, \"epoch\": 384.0, \"step\": 210048}\n",
      "\n",
      "预训练第383轮耗时： 60.41956853866577\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.1616908069760569e-05, \"epoch\": 385.0, \"step\": 210595}\n",
      "\n",
      "预训练第384轮耗时： 60.44713759422302\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.1509114198443197e-05, \"epoch\": 386.0, \"step\": 211142}\n",
      "\n",
      "预训练第385轮耗时： 60.48197531700134\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.1401320327125825e-05, \"epoch\": 387.0, \"step\": 211689}\n",
      "\n",
      "预训练第386轮耗时： 60.46630001068115\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.1293526455808457e-05, \"epoch\": 388.0, \"step\": 212236}\n",
      "\n",
      "预训练第387轮耗时： 60.45198130607605\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.1185732584491085e-05, \"epoch\": 389.0, \"step\": 212783}\n",
      "\n",
      "预训练第388轮耗时： 60.65047550201416\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.1077938713173713e-05, \"epoch\": 390.0, \"step\": 213330}\n",
      "\n",
      "预训练第389轮耗时： 60.49141979217529\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.097014484185634e-05, \"epoch\": 391.0, \"step\": 213877}\n",
      "\n",
      "预训练第390轮耗时： 60.312910079956055\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.0862350970538968e-05, \"epoch\": 392.0, \"step\": 214424}\n",
      "\n",
      "预训练第391轮耗时： 60.279048442840576\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.0754557099221601e-05, \"epoch\": 393.0, \"step\": 214971}\n",
      "\n",
      "预训练第392轮耗时： 60.524516105651855\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.064676322790423e-05, \"epoch\": 394.0, \"step\": 215518}\n",
      "\n",
      "预训练第393轮耗时： 60.38632297515869\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.0538969356586856e-05, \"epoch\": 395.0, \"step\": 216065}\n",
      "\n",
      "预训练第394轮耗时： 60.23220205307007\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.0431175485269484e-05, \"epoch\": 396.0, \"step\": 216612}\n",
      "\n",
      "预训练第395轮耗时： 60.21969652175903\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.0323381613952113e-05, \"epoch\": 397.0, \"step\": 217159}\n",
      "\n",
      "预训练第396轮耗时： 60.22520470619202\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.0215587742634744e-05, \"epoch\": 398.0, \"step\": 217706}\n",
      "\n",
      "预训练第397轮耗时： 60.40528082847595\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1.0107793871317373e-05, \"epoch\": 399.0, \"step\": 218253}\n",
      "\n",
      "预训练第398轮耗时： 60.01211738586426\n",
      "\n",
      "{\"loss\": 0.0, \"learning_rate\": 1e-05, \"epoch\": 400.0, \"step\": 218800}\n",
      "\n",
      "预训练第399轮耗时： 60.114561319351196\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random.seed(0)\n",
    "np.random.seed(0)#seed应该在main里尽早设置，以防万一\n",
    "os.environ['PYTHONHASHSEED'] =str(0)#消除hash算法的随机性\n",
    "import transformers as _\n",
    "from transformers1 import Trainer, TrainingArguments,BertTokenizer\n",
    "#from NLP_Utils import MLM_Data,train_data,blockShuffleDataLoader\n",
    "\n",
    "from NEZHA.configuration_nezha import NeZhaConfig\n",
    "from NEZHA.modeling_nezha import NeZhaForMaskedLM\n",
    "\n",
    "maxlen=32\n",
    "batch_size=128\n",
    "#vocab_file_dir = './nezha_model/vocab.txt'\n",
    "bert_dir = '/kaggle/input/pretrainbertmodel/nezha-cn-base_new/nezha-cn-base/'\n",
    "vocab_file_dir='/kaggle/input/pretrainbertmodel/nezha-cn-base_new/nezha-cn-base/vocab.txt'\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_file_dir)\n",
    "\n",
    "config = NeZhaConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=512,\n",
    ")\n",
    "\n",
    "\n",
    "#model = NeZhaForMaskedLM.from_pretrained(\"../../nezha-cn-base/\")\n",
    "model = NeZhaForMaskedLM.from_pretrained(bert_dir)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "train_MLM_data=MLM_Data(train_data,maxlen,tokenizer)\n",
    "#自己定义dataloader，不要用huggingface的\n",
    "dl=blockShuffleDataLoader(train_MLM_data,None,key=lambda x:len(x[0])+len(x[1]),shuffle=False\n",
    "                          ,batch_size=batch_size,collate_fn=train_MLM_data.collate)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./nezha_output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=400,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    save_steps=len(dl)*10000,#每10个epoch save一次\n",
    "    save_total_limit=3,\n",
    "    logging_steps=len(dl),#每个epoch log一次\n",
    "    seed=2021,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=int(450000*150/batch_size*0.03)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataLoader=dl,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainer.train()\n",
    "    trainer.save_model('./nezha_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24113.050169,
   "end_time": "2022-10-12T16:21:33.327441",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-12T09:39:40.277272",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
